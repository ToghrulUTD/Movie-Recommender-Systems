{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38fI2-ZVVo43",
        "outputId": "4bb3127d-d78d-4c74-9850-8ed15d6cefbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 latent features - 10 Epochs\n",
            "\n",
            "Train MSE: 0.5086526541757123\n",
            "Test MSE: 0.5444256163426991\n",
            "\n",
            "10 latent features - 15 Epochs\n",
            "\n",
            "Train MSE: 0.5048889300331068\n",
            "Test MSE: 0.5406031146346947\n",
            "\n",
            "15 latent features - 10 Epochs\n",
            "\n",
            "Train MSE: 0.4753186994795162\n",
            "Test MSE: 0.5277953325266466\n",
            "\n",
            "15 latent features - 15 Epochs\n",
            "\n",
            "Train MSE: 0.47338385709648656\n",
            "Test MSE: 0.5261933265332943\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Increase Memory if data is big\n",
        "# SparkContext.setSystemProperty('spark.driver.memory', '10g')\n",
        "# SparkContext.setSystemProperty('spark.executor.memory', '10g')\n",
        "\n",
        "# Start Spark Context\n",
        "sc = SparkContext(\"local\", \"MF mllib implementation\") # local machine\n",
        "\n",
        "# Load data\n",
        "data = sc.textFile(\"small_ratings.csv\")\n",
        "header = data.first() # extract header\n",
        "data = data.filter(lambda row: row != header) # filter out header\n",
        "\n",
        "# Convert the rows into Rating objectS - ((user, movie), rating) format\n",
        "ratings = data.map(lambda row: row.split(',')).map(lambda row: Rating(int(row[0]), int(row[1]), float(row[2])))\n",
        "\n",
        "# Split into Train and Test set\n",
        "train, test = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train and Evaluate accuracy for different values of params \n",
        "for rank in [10,15]:\n",
        "  for iterations in [10, 15]:\n",
        "    print(f\"{rank} latent features - {iterations} Epochs\")\n",
        "    # Define ALS Model\n",
        "    model = ALS.train(train, rank, iterations)\n",
        "    # Evaluate the model\n",
        "    x = train.map(lambda p: (p[0], p[1]))                             # convert data into ((user,movie),rating) format\n",
        "    p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))       # make predictions\n",
        "    ratesAndPreds = train.map(lambda r: ((r[0], r[1]), r[2])).join(p) # actual and predicited rating values\n",
        "    mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()  # calculate mean squared error\n",
        "    print(\"\\nTrain MSE: %s\" % mse)\n",
        "\n",
        "    # Repeat the same for test data\n",
        "    x = test.map(lambda p: (p[0], p[1]))\n",
        "    p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "    ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(p)\n",
        "    mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "    print(\"Test MSE: %s\\n\" % mse)\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and load the model\n",
        "myModelPath = 'als_recommender'\n",
        "model.save(sc, myModelPath)"
      ],
      "metadata": {
        "id": "7dj7KGN58-WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create training script"
      ],
      "metadata": {
        "id": "ORyxs1LP-UkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_recommender.py\n",
        "# Load required packages\n",
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "from pyspark import SparkContext\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create argument parser\n",
        "parser = argparse.ArgumentParser()\n",
        "# Add arguments\n",
        "parser.add_argument('--filename', type = str, default = 'small_ratings.csv')\n",
        "parser.add_argument('--rank', type = int, default = 15)\n",
        "parser.add_argument('--epochs', type = int, default = 15)\n",
        "parser.add_argument('--output_path', type = str, default = 'als_recommender')\n",
        "# Parse arguments \n",
        "args = parser.parse_args()\n",
        "\n",
        "# Increase Memory if data is big\n",
        "# SparkContext.setSystemProperty('spark.driver.memory', '10g')\n",
        "# SparkContext.setSystemProperty('spark.executor.memory', '10g')\n",
        "\n",
        "# Start Spark Context\n",
        "sc = SparkContext(\"local\", \"MF mllib implementation\") # local machine\n",
        "\n",
        "# Load data\n",
        "data = sc.textFile(args.filename)\n",
        "header = data.first() # extract header\n",
        "data = data.filter(lambda row: row != header) # filter out header\n",
        "\n",
        "# Convert the rows into Rating objectS - ((user, movie), rating) format\n",
        "ratings = data.map(lambda row: row.split(',')).map(lambda row: Rating(int(row[0]), int(row[1]), float(row[2])))\n",
        "\n",
        "# Split into Train and Test set\n",
        "train, test = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Create ALS model\n",
        "model = ALS.train(train, args.rank, args.epochs)\n",
        "# Evaluate the model\n",
        "x = train.map(lambda p: (p[0], p[1]))                             # convert data into ((user,movie),rating) format\n",
        "p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))       # make predictions\n",
        "ratesAndPreds = train.map(lambda r: ((r[0], r[1]), r[2])).join(p) # actual and predicited rating values\n",
        "mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()  # calculate mean squared error\n",
        "print(\"Train MSE: %s\" % mse)\n",
        "\n",
        "# Repeat the same for test data\n",
        "x = test.map(lambda p: (p[0], p[1]))\n",
        "p = model.predictAll(x).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(p)\n",
        "mse = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "print(\"Test MSE: %s\" % mse)\n",
        "\n",
        "if os.path.isdir(args.output_path):\n",
        "  shutil.rmtree(args.output_path)\n",
        "model.save(sc, args.output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51qka8N9DeD",
        "outputId": "41aff0dd-cbd8-4106-8095-ccf027c4f78e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_recommender.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_recommender.py --rank 15 --epochs 15 --output_path 'recommender_15_!5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yBS-0wD_qB_",
        "outputId": "123d9b03-4eae-444e-ddab-9f286dd8b07d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/02/27 08:43:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/02/27 08:43:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/02/27 08:43:12 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 2): Attempting to kill Python Worker\n",
            "23/02/27 08:43:16 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 3): Attempting to kill Python Worker\n",
            "Train MSE: 0.4763550563982868\n",
            "Test MSE: 0.5273373229301271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile recommend.py \n",
        "from pyspark.mllib.recommendation import  MatrixFactorizationModel\n",
        "from pyspark import SparkContext\n",
        "import argparse\n",
        "\n",
        "# Create argument parser, add and parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model_path', type = str, help = 'path of model artifact')\n",
        "parser.add_argument('--user_id', type = int , default = 90, help = 'id of the currect user')\n",
        "parser.add_argument('--numRecs', type = int , default = 10, help = 'number of recommendations')\n",
        "args = parser.parse_args()\n",
        "userId = args.user_id\n",
        "numRecs = args.numRecs  # number of movies to recommend\n",
        "\n",
        "# Start Spark Context\n",
        "sc = SparkContext(\"local\", \"MF Recommender from saved model\") # local machine\n",
        "\n",
        "# Load the model\n",
        "savedModel = MatrixFactorizationModel.load(sc, args.model_path)\n",
        "\n",
        "# Generate top recommendations for a user\n",
        "topRecommendations = savedModel.recommendProducts(userId, numRecs)\n",
        "print(\"\\n\\nTop recommendations for user \" + str(userId) + \":\")\n",
        "for recommendation in topRecommendations:\n",
        "    print(recommendation.product, recommendation.rating)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohcjcAf7udjM",
        "outputId": "76a372cc-c251-4c78-f5b0-af41ac631db8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing recommend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python recommend.py --model_path '/content/recommender_15_!5' --user_id 90 --numRecs 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozdOlMux7oKG",
        "outputId": "b0f80077-d523-45b4-b596-8d9d2ace50d4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/02/27 08:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/02/27 08:53:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModel: User factor does not have a partitioner. Prediction on individual records could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModel: User factor is not cached. Prediction could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModel: Product factor does not have a partitioner. Prediction on individual records could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModel: Product factor is not cached. Prediction could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModelWrapper: User factor does not have a partitioner. Prediction on individual records could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModelWrapper: User factor is not cached. Prediction could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModelWrapper: Product factor does not have a partitioner. Prediction on individual records could be slow.\n",
            "23/02/27 08:53:25 WARN MatrixFactorizationModelWrapper: Product factor is not cached. Prediction could be slow.\n",
            "\n",
            "\n",
            "Top recommendations for user 90:\n",
            "4900 4.583015675820736\n",
            "7044 4.54327651614648\n",
            "5856 4.515123870327188\n",
            "259 4.486331364381153\n",
            "710 4.438434139099747\n",
            "1173 4.431765573816167\n",
            "1175 4.418523519267536\n",
            "734 4.416902274118808\n",
            "1127 4.415320726625371\n",
            "1174 4.391155706601188\n",
            "317 4.3890453194046914\n",
            "1115 4.346968392823182\n",
            "2241 4.323169002769236\n",
            "525 4.301230520248188\n",
            "1199 4.267401513290922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIPda7HQ7t-q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SojfC3J676_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}