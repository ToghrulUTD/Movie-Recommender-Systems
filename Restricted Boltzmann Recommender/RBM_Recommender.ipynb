{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restricted Boltzman Machines\n",
        "\n",
        "Basic structure of RBM can be considered a single layer of neural network, a single block connecting to the input layer. Input layer and Hidden layer communicates with each other two-way (forward and backward). Input layer or visibile units is denoted as $v$ and hidden layer or hidden units are denoted by $h$. Two-way communication means that we can calculate $p(v|h)$ and $p(h|v)$. For most discussions, we are talking about Bernoulli RBMs, meaning that both input and hidden units can only take values 0 and 1, such as click or not, signup or not, hence most applications are web based. However, this constraint can be relaxed.\n",
        "\n",
        "* How to calculate $h$ from $v$: \n",
        "\\begin{aligned}\n",
        "\\text{Vector form:}\n",
        "\\quad &p(h=1|v) = \\sigma(W^Tv + c)\\\\\n",
        "\\text{Scalar form:}\\quad &p(h_j=1|v)= \\sigma(\\sum_{i=1}^D W_{ij}v_i + c_j) \\quad \\text{i = 1,..,D}\\quad\\text{j = 1,..,M}\\\\\n",
        "&len(v) = D \\quad len(h)=M\n",
        "\\end{aligned}\n",
        "\n",
        "where $W$ and $c$ are weights and biases of the hidden layer. One important feature is that $W$ is the shared weight for 2-way calculation. Similarly, we have\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{Vector form:}\n",
        "\\quad &p(v=1|h) = \\sigma(Wh + b)\\\\\n",
        "\\text{Scalar form:}\\quad &p(v_i=1|v)= \\sigma(\\sum_{j=1}^D W_{ij}h_j + b_i) \\quad \\text{i = 1,..,D}\\quad\\text{j = 1,..,M}\n",
        "\\end{aligned}\n",
        "\n",
        "This model only gives us the probabilities. $h$ is the random variable, it does not have specific value, but we can draw samples from this distribution.\n",
        "\n",
        "We can relaxt 0,1 restriction of the rbm by scaling the values into 0-1 interval. It is then possible to round the values to 0,1 if we want to work with bernoulli variables. However, there is no restriction on $v$ not taking a value between 0 and 1 for calculating $p(h=1|v)$. Another way to relax the restriction on $h$ is to use the probabilities instead of the sampled value. For instance, we can write\n",
        "\n",
        "\\begin{aligned}\n",
        "\\hat{h}=p(h=1|v) = \\sigma(W^Tv+c)\\\\\n",
        "p(v'=1|h)= \\sigma(W\\hat{h} + b )\n",
        "\\end{aligned}\n",
        "\n",
        "This is similar to the way autoencoder works, if we assume that autoencoder has a sigmoid activation at both in hidden and output layers and it shares the weight metrics between its two layers, then going forward in this autoencoder is same as going to the hidden layer and going back to the visible layer in RBM.\n",
        "\n",
        "If we observe how close input $v$ is close to the output $v'$ by kl-divergence or some crossentropy, this loss function will go down as we train despite directly optimizing for this.\n",
        "\n"
      ],
      "metadata": {
        "id": "SDoRm_nT7ahy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation for Boltzman Machines\n",
        "\n",
        "Think about how brain works: Mostly, neurons receives signals from the environment and we integrate that information using our internal pattern recognition in unsupervised manner. \n",
        "\n",
        "RBM is similar to this in a way that we are not imposing any structure on it, where everything can be connected to everything, and some units are hidden and some are visible. Visible  neurons can be considered as connected to outer world like sensory neurons, hidden neurans are not directly connected to outer world but are connected to the sensory neurons.\n",
        "\n",
        "Ludwig Bolzman - statistical mechanics - intuition is that physical systems tend to go to an equilibrium state like from high energy to low energy state. Energy in a boltzman machine is given as \n",
        "$$ E = -\\Bigg(\\sum_{ij} W_{ij}s_is_j + \\sum_{i}b_is_i\\Bigg)$$\n",
        "\n",
        "The goal in training boltzman machine is simply to find a thermal equilibrium (maybe not simple as this but anyways), this expression given above looks very similar to the neural network calculation.\n",
        "\n",
        "In restricted boltzman machines, visible units and hidden units are connected to each other but not neurons of themselves.\n",
        "\n",
        "Energy in the RBM is defined as\n",
        "\n",
        "\\begin{aligned}\n",
        " E(v,h) &= -\\Bigg(\\sum_{i=1}^D\\sum_{j}^M W_{ij}v_ih_j + \\sum_{i=1}^Db_iv_i + \\sum_{j=1}^M c_jh_j\\Bigg) \\quad \\text{or} \\\\\n",
        "E(v,h) &= -\\Bigg(v^TWh + b^Tv +  c^Th\\Bigg)\\\\\n",
        "\\text{where}\\quad v &= NxDxK \\quad h = NxDxK\\quad W=DxKxM\\quad\n",
        "b = DxK \\quad c=M \\quad \\text{(dimensions)}\n",
        "\\end{aligned}\n",
        "\n",
        "Extra $K$ dimension represents the one-hot encoded data. For instance, if we are working with ratings from 1 to 5, we can convert these ratings to a one-hot encoded vector of size 5. For instance rating of 3 would be encoded as [0,0,1,0,0]. For now, we can ignore that dimension as we are not considering non-binary data for mathematical derivations.\n",
        "\n",
        "Next Steps:\n",
        "1. Show how this energy function leads to a probabilistic model\n",
        "\n",
        "2. Show how this probabilistic model leads us to the simple neural network equations shown at the beginning."
      ],
      "metadata": {
        "id": "PMUFFrAlCqsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy --> Probability Model\n",
        "\n",
        "The join probability of $v$ and $h$ is proportional to the negative exponent of energy function.\n",
        "\n",
        "\\begin{aligned}\n",
        "p(v,h)&\\propto e^{-E(v,h)}\\\\\n",
        "p(v,h)& = \\frac{1}{Z}e^{-E(v,h)}\\quad Z = \\sum_{v}\\sum_{h}e^{-E(v,h)} \\\\\n",
        "\\text\n",
        "{so that:}&\\quad \\sum_{v}\\sum_{h}e^{-E(v,h)} p(v,h)= 1\n",
        "\\end{aligned}\n",
        "\n",
        "Why we define probability this way? In statistical mechanics, we have a probability $p_i$ that a system is in a microstate with  energy $E_i$. Through some physics, it can be shown that $p_i$ is proportional to exponent of minus $E_i$ divided by $kT$ where $T$ is a temperature $$ p_i = e^{-E_i/(kT)}\\\\\n",
        "p_i = \\frac{1}{Z} e^{-E_i/(kT)}\\quad Z = \\sum_{i}e^{E_i/(kT)}.$$\n",
        "\n",
        "$Z$ is a normalizing constant and is found by making the sum of probabilities equal to 1. \n",
        "\n",
        "## Challenge\n",
        "A major computation challenge with the RBM equations is that we can not calculate this in a reasonable amount of time. \n",
        "$$p(v,h) = \\frac{1}{Z}e^{-E(v,h)}\\quad Z = \\sum_{v}\\sum_{h}e^{-E(v,h)}$$\n",
        "\n",
        "If $v$ have $D$ dimension and $h$ has $M$ dimension, there are 2^{M+D} possible combinations of bernoulli variables. It can not be calculated practically."
      ],
      "metadata": {
        "id": "HhLro2WoJKyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Model --> Neural Network Equations\n",
        "\n",
        "Using baes rule, we have\n",
        "\\begin{aligned}\n",
        "p(v|h) = p(v,h)/p(h) = p(v,h)/\\sum_{v}p(v,h)\\\\\n",
        "p(h|v) = p(v,h)/p(v)= p(v,h)/\\sum_{h}p(v,h).\n",
        "\\end{aligned}\n",
        "\n",
        "Using $E(v,h) = -(v^TWh + b^Tv +  c^Th)$, we can write this as \n",
        "\\begin{aligned}\n",
        "p(v,h) &= \\frac{1}{Z}\\exp(v^TWh + b^Tv +  c^Th)\\\\\n",
        "p(v) &= \\frac{1}{Z}\\sum_{h}\\exp(v^TWh + b^Tv +  c^Th)\\\\\n",
        "\\implies p(h|v) &= \\frac{\\exp(v^TWh + b^Tv +  c^Th)}{\\sum_{h}\\exp(v^TWh + b^Tv +  c^Th)}\n",
        "\\end{aligned}\n",
        "\n",
        "As we can see, $Z$'s are cancelled out, but we still need to sum over $h$ which is intractable. To simplify further, we can ignore the denominator as it is another normalizing constant. Considering it another $Z'$, we have\n",
        "\\begin{aligned}\n",
        "p(h|v) &= \\frac{1}{Z'}\\exp\\Bigg(\\sum_{i=1}^D\\sum_{j}^M W_{ij}v_ih_j + \\sum_{i=1}^Db_iv_i + \\sum_{j=1}^M c_jh_j\\Bigg)\\\\\n",
        "&= \\frac{1}{Z'}\\exp(\\sum_{i=1}^Db_iv_i) \\prod_{j=1}^M\\exp\\Bigg(h_j\\Bigg\\{\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg\\}\\Bigg)\\\\\n",
        "&=\\frac{1}{Z''}\\prod_{j=1}^M\\exp\\Bigg(h_j\\Bigg\\{\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg\\}\\Bigg)\n",
        "\\end{aligned}\n",
        "Since each of the $j$ term is independent, we can write\n",
        "\\begin{aligned}\n",
        "p(h_j|v) &= \\frac{1}{Z'''}\\exp\\Bigg(h_j\\Bigg\\{\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg\\}\\Bigg)\\\\\n",
        "\\implies p(h_j=1|v) &= \\frac{1}{Z'''}\\exp\\Bigg(\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg)\\\\\n",
        "\\implies p(h_j=0|v) &= \\frac{1}{Z'''} .\n",
        "\\end{aligned}\n",
        "Since $p(h|v)$ is a valid probability, we must have\n",
        "\\begin{aligned}\n",
        "&p(h_j=0|v) + p(h_j=1|v)=1 \\quad \\implies Z''' = 1+\\exp\\Bigg(\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg)\n",
        "\\end{aligned}\n",
        "\n",
        "Plugging this value into the probability function, we get \n",
        "\n",
        "\\begin{aligned}\n",
        "p(h=1|v) = \\frac{\\exp\\Bigg(\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg)}{1+\\exp\\Bigg(\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg)} &= \\sigma\\Bigg(\\sum_{i=1}^D W_{ij}v_i + c_j\\Bigg) =\\sigma(W^Tv + c)\n",
        "\\end{aligned}\n"
      ],
      "metadata": {
        "id": "XrlFm914ViyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Train - Objective and Interpretation\n",
        "\n",
        "We would like to solve Maximum Likelihood Problem where we maximize $p(v)$ given $W$, $b$ and $c$:\n",
        "$$W,b,c = argmax_{W,b,c} \\log p(v; W,b,c).$$\n",
        "\n",
        "The problem with this problem is that the cost function is not tractable because the sum takes to long. \n",
        "\n",
        "We can instead define new quantity called free energy:\n",
        "$$F(v) = -\\log \\sum_{h}e^{-E(v,h)}.\\\\\n",
        "\\sum_{h}e^{-E(v,h)} = e^{-F(v)}.\\\\\n",
        "p(v) = \\frac{1}{Z}\\sum_{h}e^{-E(v,h)} = \\frac{1}{Z}e^{-F(v)} \\quad \\text{and } Z = \\sum_{v}e^{-F(v)}$$\n",
        "\n",
        "Let us pretend, we can do gradient descent such as\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\log p(v)}{\\partial\\theta} = \\frac{\\partial}{\\partial \\theta}\\Bigg(-F(v) - \\log Z\\Bigg)=-\\frac{\\partial F(v)}{\\partial \\theta}  - \\frac{1}{Z}\\frac{\\partial Z}{\\partial \\theta}\n",
        "\\end{aligned}\n",
        "Plugging in the expression for $Z$ and rearranging the expression, we get\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\log p(v)}{\\partial \\theta}= -\\frac{\\partial F(v)}{\\partial \\theta}  - \\frac{1}{Z}\\sum_{v'}\\frac{\\partial}{\\partial\\theta}e^{-F(v')}\\\\\n",
        "= -\\frac{\\partial F(v)}{\\partial \\theta} + \\sum_{v'}\\frac{1}{Z}e^{-F(v')}\\frac{\\partial F(v')}{\\partial\\theta}\\\\\n",
        "=-\\frac{\\partial F(v)}{\\partial \\theta}  + \\sum_{v'}p(v')\\frac{\\partial F(v')}{\\partial\\theta}.\n",
        "\\end{aligned}\n",
        "Flipping the sing on both sides (maximizing log likelihood is same as minimizing negative log likelihood), we get\n",
        "$$-\\frac{\\partial \\log p(v)}{\\partial \\theta} = \\underbrace{\\frac{\\partial F(v)}{\\partial \\theta}}_{\\text{positive phase}}  - \\underbrace{\\sum_{v'}p(v')\\frac{\\partial F(v')}{\\partial\\theta}}_{\\text{negative phase}} .$$\n",
        "\n",
        "The gradient is made of positive and negative phases. If we are doing stochastic gradient descen (one feature vector at a time), only the first term depends on $v$, and the role it plays is to makes $v$ more likely, i.e., makes our model more likely to produce these $v$ which are the actual observations.\n",
        "Second term does not depend on the feature vector $v$ rather sums over all possible values of $v$. The role of the second term is to make all values of $v$ less likely, and it is weighted by the probability of those values of $v$. So the combined effort is to make the actual value (what we see) more likely while making all other values less likely, especially the values of $v$ which are currently likely but have not been yet observed. So if the model currently performs bad and gives a high probability to a value not observed before, the second term reduces its probability more by weighting it by its current probability"
      ],
      "metadata": {
        "id": "HDLzdbwHhbWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to train - further simplification\n",
        "\n"
      ],
      "metadata": {
        "id": "jYR6OZvGqIH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the negative phase expression is the expected value of $f(v')$ (gradient of free energy), which allows us to write\n",
        "$$ -\\frac{\\partial \\log p(v)}{\\partial \\theta} = f(v')  - E(f(v)) \\\\\n",
        "E(f(v)) = \\sum_{v'}p(v')f(v')$$\n",
        "We can approximate the expected value by sample mean $$ E(f(v)) = \\frac{\\sum_{n}f(v_n)}{N}$$.\n",
        "\n",
        "The way we generate this samples is Markov Chain Monte Carlo Method (MCMC) - especially Gibbs sampling method. The idea is as following:\n",
        "* Given: $v_1$\n",
        "* Calculate $p(h_1|v_1)$\n",
        "* Sample $h_1\\sim p(h_1|v_1)$\n",
        "* Calculate $p(v_2|h_1)$\n",
        "* Sample $v_2\\sim p(v_2|h_1)$\n",
        "* . . . . .\n",
        "* $v_{\\infty}$ is a sample from $p(v)$\n",
        "\n",
        "The idea is that as we repeat this procedure infinite times $v$ is guaranted to come from true probability distribution.\n",
        "However, usually even 1 step is enough.\n",
        "* $v → p(h|v) → h\\sim p(h|v)→p(v'|h)→v'\\sim p(v'|h)$ or in short $v→h→v'$\n",
        "\n",
        "So now, the approximated gradient becomes the difference between free energy gradients (input v and sample v') $$-\\frac{\\partial \\log p(v)}{\\partial \\theta}  = \\frac{\\partial F(v)}{\\partial \\theta} - \\frac{\\partial F(v')}{\\partial \\theta}$$\n",
        "which is called contrastive divergence ($CD-k$) with $k$ steps of Gibbs sampling performed to get $v'$. The above method is $CD-1$, and it can be changed to see if the result improves.\n",
        "\n",
        "Pseudo Algorithm:\n",
        "```\n",
        "for v in dataset:\n",
        "  p(h=1|v) = sigmoid(W.T.dot(v)+c) # visible to hidden\n",
        "  h = sample_from(p(h=1|v))\n",
        "  p(v'=1|h) = sigmoid(W.dot(h)+b) # hidden to visible\n",
        "  v' = sample_from(p(v'=1|h))\n",
        "  param = param - learning_rate(grad(F(v))-grad(F(v'))) # find approximated grad\n",
        "```\n",
        "\n",
        "Since gradient step is handled automatically by the libraries such as Tensorflow, we just need to define the  loss function (this is not the real loss, just the gradient is what we need) as: $$L = F(v)-F(v')$$.\n",
        "\n",
        "We previously defined free energy $F(v)$ as $$F(v) = -log \\sum_{h}e^{-E(v,h)}$$\n",
        "\n",
        "One way to calculate this in a tractable way is as follows:\n",
        "\\begin{aligned}\n",
        "F(v) &= -log \\sum_{h}e^{v^TWh + b^Tv +  c^Th} = -b^Tv - log \\sum_{h}e^{v^TWh  + c^Th}\\\\\n",
        "&=-b^Tv - log \\sum_{h}\\exp\\Bigg\\{\\sum_{j=1}^{M} v^TW_{:,j}h_j + c_jh_j\\bigg\\} = -b^Tv - log \\sum_{h}\\prod_{j=1}^M \\exp h_j\\Bigg(v^TW_{:,j} + c_j\\bigg) \n",
        "\\end{aligned}\n",
        "\n",
        "Using Sum-Product rule, we can rearrange the above expression as \n",
        "\\begin{aligned}\n",
        "F(v) &= -b^Tv - log \\prod_{j=1}^M\\sum_{h} \\exp h_j\\Bigg(v^TW_{:,j} + c_j\\bigg) = -b^Tv - \\sum_{j=1}^Mlog \\sum_{h_j\\in\\{0,1\\}} \\exp h_j\\Bigg(v^TW_{:,j} + c_j\\bigg) \\\\\n",
        "&= -b^Tv - \\sum_{j=1}^M \\log \\Bigg\\{1+ \\exp\\Bigg(v^TW_{:,j} + c_j\\bigg)\\Bigg\\}.\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "Finally, free energy is no longer is intractable, the only summation is over j and it is linear, not exponential."
      ],
      "metadata": {
        "id": "-m2sGVCrADE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application to Movielens Recommendations\n",
        "\n",
        "Bernoulli RBM does not work since the input is not within the range of [0,1].  In the paper, authors have made the visible units a $k$-class categorical distribution rather than bernoulli distribution. So 10 rating values from 0.5 and 5 are ecnoded to a one-hot vector or dimension 10 for each input rating. The rating 3.5 for instance will be $[0,0,0,0,0,0,1,0,0,0]$. They then use softmax to calculate the probabilities.\n",
        "\n",
        "The calculations become as follows:\n",
        "$$p(h_j=1|v) = \\sigma(\\sum_{k=1}^K\\sum_{i=1}^D W_{ij}^k v_i^k + c_j)\\\\\n",
        "p(v_i^k = 1|h) = softmax(\\sum_{j=1}^M W^k_{ij}h_{j} + b_i)\n",
        "$$\n",
        "\n",
        "* Vector Shapes\n",
        "  *   **h** vector of size M (number of hidden units)\n",
        "  *   **V** 2D array of size D x K - each movie needs its own categorical distribution, so D is the number of movies (number of input units) and K is the number of possible ratings.\n",
        "  *   **W** 3D array of size D x K x M\n",
        "  *   **b** 2D array of size D x K\n",
        "  *   **c** vector of size M\n",
        "\n",
        "In movielens example, we also have $N$ dimension which represents the number of samples (number of users) or batch size, so the size of $V$ and $H$ will be $N x D x K$ and $N x M$.\n",
        "\n",
        "Note that when we sum over both $K$ and $D$, it is not actually dot product ( but double dot product if there is such a thing). \n",
        "\n",
        "With categorical inputs, the free energy expression becomes $$F(v) = -b^Tv - \\sum_{j=1}^M \\log \\Bigg\\{1+ \\exp\\Bigg(v^TW_{:,:,j} + c_j\\bigg)\\Bigg\\}\n",
        "$$\n",
        "\n",
        "We have implicit double-dot operation over $b$ and $W$ in the above expression. To do this, we will use $tf.tensordot$ which allows multiplying 3 dimensional tensors.\n",
        "\n",
        "## How to deal with missing ratings\n",
        "\n",
        "$v_{i,k} = 1$ if a user rates the movie $i$ with $k$ (k corresponds to k/2 rating here).\n",
        "If rating is missing the one-hot vector will be an array of all zeros. If we have a missing rating, we also want the sampled $v'(i)$ to be an array of all zeros. That way free energy at both $v(i)$ and $v'(i)$ will have the same value, and the difference hence the gradient will be 0, and will not change the parameters due to the missing rating.\n",
        "\n",
        "## Rating Predictions\n",
        "\n",
        "After getting probabilities for each rating value (with softmax), we can get a weighted average of the ratings to get the final predicted rating (method used in the paper).\n",
        "\n",
        "Or predicted value may be found by argmax of softmax outputs."
      ],
      "metadata": {
        "id": "5WvSJun9R09Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "1. Preoricess ratings dataframe into sparse matrix\n",
        "\n",
        "2. Build required functions and RBM class\n",
        "\n",
        "3. Train the model"
      ],
      "metadata": {
        "id": "JPrirXtZ1-Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data to sparse matrices\n",
        "\n",
        "The preprocess_sparse.py will load and preprocess data by resetting the movie and user ids. It then creates train and test dataframes from the full dataset, creates sparse matrices for train and test set where rows and columns represents users and movies respectively, and each cell value $x_{ij}$ represent user i's rating for movie j."
      ],
      "metadata": {
        "id": "NqJ0xcvs2Nl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess_sparse.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "# Create an argument parser\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Add arguments\n",
        "parser.add_argument('--input', type=str, default='small_ratings.csv', help='Inpute file path')\n",
        "parser.add_argument('--test_size', type=float, default=0.8, help='Size/fraction of the test set')\n",
        "\n",
        "# Parse arguments\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Execution time\n",
        "start_time = time.time()\n",
        "\n",
        "# Load ratings data\n",
        "df = pd.read_csv(args.input)\n",
        "\n",
        "# Create a mapping to reset user and movie Ids\n",
        "movies = {movie:i for i, movie in enumerate(set(df.movieId.values))}\n",
        "users = {user:i for i, user in enumerate(set(df.userId.values))}\n",
        "\n",
        "# Reset movie and user Ids\n",
        "df['userId'] = df['userId'].apply(lambda x: users[x])\n",
        "df['movieId'] = df['movieId'].apply(lambda x: movies[x])\n",
        "print('\\nLoad and preprocess complete...')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movieId.max() + 1 # number of movies\n",
        "\n",
        "# Split data into train and test by 80/20 split\n",
        "df = shuffle(df)\n",
        "cutoff = int(args.test_size*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "print(f'\\nTrain test split complete...')\n",
        "\n",
        "# Create a sparse rating (users x movies) matrices for train data\n",
        "print('\\nCreating sparse user-movie rating matrixes train and test sets...')\n",
        "start =  time.time()\n",
        "s_train = lil_matrix((N,M)) # lil matrix is best when we need to modify the matrix frequentyly\n",
        "print('\\nUpdating sparse train matrix...')\n",
        "count = 0\n",
        "for row in df_train.values:\n",
        "  count+=1\n",
        "  if count%100000==0: print(\"processed: %.3f \"  % (float(count)/cutoff))\n",
        "  user,movie = row[0], row[1]  # user and movie id\n",
        "  s_train[user,movie] = row[2] # rating\n",
        "\n",
        "# Convert the lil matrix into compressed sparse row matrix\n",
        "s_train = s_train.tocsr() # csr format is best for matrix-vector multiplication operations\n",
        "save_npz(\"train_sparse.npz\", s_train)\n",
        "print(f'\\nExecution time = %.3f seconds' %(time.time() - start))\n",
        "\n",
        "# Create a sparse rating (users x movies) matrices for test data\n",
        "start =  time.time()\n",
        "s_test = lil_matrix((N,M)) \n",
        "print('\\nUpdating sparse test matrix...')\n",
        "count = 0\n",
        "for row in df_test.values:\n",
        "  count+=1\n",
        "  if count % 50000==0: print(\"processed: %.3f \" % (float(count)/len(df_test)))\n",
        "  user,movie = row[0], row[1]  # user and movie id\n",
        "  s_test[user,movie] = row[2] # rating\n",
        "\n",
        "# Convert the lil matrix into compressed sparse row matrix\n",
        "s_test = s_test.tocsr() \n",
        "save_npz(\"test_sparse.npz\", s_test)\n",
        "print(f'\\nExecution time = %.3f seconds' %(time.time() - start))\n",
        "\n",
        "# Print total loading and preprocessing time\n",
        "print(f'\\nTotal execution time = %.3f seconds' %(time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahej07HE25FS",
        "outputId": "cf0380ad-d58e-4c41-9b14-9d91db5887a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_sparse.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the train script\n",
        "!python preprocess_sparse.py --input 'small_ratings.csv' --test_size 0.8 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvEnJQIM1NAJ",
        "outputId": "2f598fde-6700-4334-f8bf-8b118acba117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Load and preprocess complete...\n",
            "\n",
            "Train test split complete...\n",
            "\n",
            "Creating sparse user-movie rating matrixes train and test sets...\n",
            "\n",
            "Updating sparse train matrix...\n",
            "processed: 0.053 \n",
            "processed: 0.106 \n",
            "processed: 0.159 \n",
            "processed: 0.212 \n",
            "processed: 0.266 \n",
            "processed: 0.319 \n",
            "processed: 0.372 \n",
            "processed: 0.425 \n",
            "processed: 0.478 \n",
            "processed: 0.531 \n",
            "processed: 0.584 \n",
            "processed: 0.637 \n",
            "processed: 0.690 \n",
            "processed: 0.744 \n",
            "processed: 0.797 \n",
            "processed: 0.850 \n",
            "processed: 0.903 \n",
            "processed: 0.956 \n",
            "\n",
            "Execution time = 14.656 seconds\n",
            "\n",
            "Updating sparse test matrix...\n",
            "processed: 0.106 \n",
            "processed: 0.212 \n",
            "processed: 0.319 \n",
            "processed: 0.425 \n",
            "processed: 0.531 \n",
            "processed: 0.637 \n",
            "processed: 0.744 \n",
            "processed: 0.850 \n",
            "processed: 0.956 \n",
            "\n",
            "Execution time = 2.942 seconds\n",
            "\n",
            "Total execution time = 24.467 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Model"
      ],
      "metadata": {
        "id": "r8pXoWU-PFDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_sparse.py\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from scipy.sparse import csr_matrix, load_npz\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import argparse\n",
        "\n",
        "# Create an argument parser\n",
        "parser = argparse.ArgumentParser()\n",
        "# Add arguments\n",
        "parser.add_argument('--epochs', type = int, default  = 10, help = 'Number of epochs')\n",
        "parser.add_argument('--batch_size', type = int, default = 128, help = 'Batch size')\n",
        "parser.add_argument('--n_neurons', type = int, default = 256, help = 'Size of Hidden layer')\n",
        "parser.add_argument('--cd_k', type = int, default = 1, help = 'Number of steps for Gibbs sampling')\n",
        "parser.add_argument('--learning_rate', type = float, default = 0.01, help = 'Learning rate for training')\n",
        "parser.add_argument('--output_file', type = str, default = 'weights.pkl', help = 'path for saving model weights')\n",
        "\n",
        "# Parse the arguments\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Load sparse train and test matrices\n",
        "ratings = load_npz('train_sparse.npz')\n",
        "test_ratings = load_npz('test_sparse.npz')\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "D = ratings.shape[1]  # Number of movies\n",
        "K = 10  # Number of rating categories (e.g. 1-5 stars)\n",
        "\n",
        "# Access the arguments and assign to parameters \n",
        "M = args.n_neurons  # Number of hidden units\n",
        "batch_size = args.batch_size  # Batch size for training\n",
        "learning_rate = args.learning_rate  # Learning rate for Adam optimizer\n",
        "cd_k = args.cd_k # The number of steps for gibbs sampling\n",
        "epochs = args.epochs  # Number of epochs to train\n",
        "\n",
        "# Define auxilary dot product functions for forward and backward layer calculations \n",
        "def dot1(V, W):\n",
        "    # V is N x D x K (batch of visible units)\n",
        "    # W is D x K x M (weights)\n",
        "    # returns N x M (hidden layer size)\n",
        "    return tf.tensordot(V, W, axes=[[1,2], [0,1]])\n",
        "\n",
        "def dot2(H, W):\n",
        "    # H is N x M (batch of hiddens)\n",
        "    # W is D x K x M (weights transposed)\n",
        "    # returns N x D x K (visible)\n",
        "    return tf.tensordot(H, W, axes=[[1], [2]])\n",
        "\n",
        "# Define the RBM model\n",
        "class RBM(tf.keras.Model):\n",
        "    def __init__(self, D, K, M):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = tf.Variable(tf.random.normal([D, K, M], stddev= np.sqrt(2/M)))\n",
        "        self.b = tf.Variable(tf.zeros([D, K], dtype = tf.float32))\n",
        "        self.c = tf.Variable(tf.zeros([M],dtype = tf.float32))\n",
        "\n",
        "    def forward_hidden(self, v):\n",
        "        return tf.nn.sigmoid(dot1(v, self.W) + self.c)\n",
        "\n",
        "    def forward_logits(self, v):\n",
        "        Z = self.forward_hidden(v)\n",
        "        return dot2(Z, self.W) + self.b\n",
        "\n",
        "    def forward_output(self, v):\n",
        "        return tf.nn.softmax(self.forward_logits(v))\n",
        "\n",
        "    def call(self, v):\n",
        "        p_h_given_v = self.forward_hidden(v) #tf.nn.sigmoid(dot1(v, self.W) + self.c)\n",
        "        r = tf.random.uniform(shape=tf.shape(input=p_h_given_v))\n",
        "        h = tf.cast(r < p_h_given_v, dtype=tf.float32)\n",
        "        return h, p_h_given_v\n",
        "        \n",
        "    def free_energy(self, v):\n",
        "        first_term = -tf.reduce_sum(input_tensor=dot1(v, self.b))\n",
        "        second_term = -tf.reduce_sum(\n",
        "            # tf.log(1 + tf.exp(tf.matmul(V, self.W) + self.c)),\n",
        "            input_tensor=tf.nn.softplus(dot1(v, self.W) + self.c), axis=1)\n",
        "        return first_term + second_term\n",
        "\n",
        "    def contrastive_divergence(self, v, mask, k=cd_k):\n",
        "        h, p_h_given_v = self.call(v)\n",
        "        for i in range(k):\n",
        "            logits = dot2(h, self.W) + self.b\n",
        "            cdist = tfp.distributions.Categorical(logits = logits)\n",
        "            v = cdist.sample() # shape is (N, D)\n",
        "            v = tf.one_hot(v, depth=K)*mask # turn it into (N, D, K)\n",
        "            h, p_h_given_v = self.call(v)\n",
        "        return v\n",
        "    \n",
        "# Define the RBM model object and optimizer\n",
        "rbm = RBM(D, K, M)\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Define the training loop\n",
        "n_batch = ratings.shape[0]//batch_size\n",
        "train_test_error = {'train_sse':[], 'test_sse': []}\n",
        "for epoch in range(epochs):\n",
        "    sse_sum, tsse_sum = 0, 0\n",
        "    sse_count, tsse_count = 0, 0\n",
        "    for i in range(0, ratings.shape[0], batch_size):\n",
        "        # Extract the current batch of ratings data and create mask\n",
        "        x = ratings[i:i+batch_size].toarray()\n",
        "        x_hot = tf.one_hot(x*2 - 1, K)\n",
        "        mask2d = tf.cast(x > 0, tf.float32)\n",
        "        mask = tf.stack([mask2d]*K, axis=-1)\n",
        "\n",
        "        # Perform one step of contrastive divergence\n",
        "        with tf.GradientTape() as tape:\n",
        "            v = rbm.contrastive_divergence(x_hot, mask)\n",
        "            cost = tf.reduce_mean(rbm.free_energy(x_hot)) - tf.reduce_mean(rbm.free_energy(v))\n",
        "\n",
        "        # Update the model parameters using the Adam optimizer\n",
        "        grads = tape.gradient(cost, rbm.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, rbm.trainable_variables))\n",
        "\n",
        "        # Calculate train sse\n",
        "        one_to_ten = tf.constant((np.arange(10) + 1).astype(np.float32) / 2)\n",
        "        output_visible = rbm.forward_output(x_hot)\n",
        "        pred = tf.tensordot(output_visible, one_to_ten, axes=[[2], [0]])\n",
        "        sse = tf.reduce_sum(mask2d * (x - pred) * (x - pred))\n",
        "        sse_sum = sse_sum + sse\n",
        "        sse_count = sse_count + tf.reduce_sum(mask2d)\n",
        "\n",
        "        # Calculate the test sse\n",
        "        x_test = test_ratings[i:i+batch_size].toarray()\n",
        "        test_x_hot = tf.one_hot(x_test*2 - 1, K)\n",
        "        test_mask2d = tf.cast(x_test > 0, tf.float32)\n",
        "        tsse = tf.reduce_sum(test_mask2d * (x_test - pred) * (x_test - pred))\n",
        "        tsse_sum = tsse_sum + tsse\n",
        "        tsse_count = tsse_count + tf.reduce_sum(test_mask2d)\n",
        "        train_sse = sse_sum/sse_count\n",
        "        test_sse = tsse_sum/tsse_count\n",
        "\n",
        "    # Append the costs to train_test_error dictionary\n",
        "    train_test_error['train_sse'].append(train_sse.numpy())\n",
        "    train_test_error['test_sse'].append(test_sse.numpy())\n",
        "    print(f'Epoch {epoch + 1}: Train SSE = {train_sse.numpy()} and Test SSE = {test_sse.numpy()}') \n",
        "\n",
        "# Plot mean squared error by epoch\n",
        "try:\n",
        "  assert epochs > 0\n",
        "  plt.figure(figsize = (10,8))\n",
        "  plt.plot(np.arange(epochs), train_test_error['train_sse'], label = 'Train SSE')\n",
        "  plt.plot(np.arange(epochs), train_test_error['test_sse'], label = 'Test SSE')\n",
        "  plt.legend()\n",
        "  plt.savefig('model_performance')\n",
        "except: \n",
        "  print('No history')\n",
        "# Save the model weights\n",
        "joblib.dump(rbm.get_weights(), args.output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICLRACVHNcnq",
        "outputId": "ee5e8b68-1778-4778-c6e8-e83d0909f304"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_sparse.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the train script\n",
        "!python train_sparse.py --epochs 10 --batch_size 256 --cd_k 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkxPnEbi0jo0",
        "outputId": "53257d48-d53f-4782-99cf-b895118e758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 00:22:22.287417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 00:22:23.558941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 00:22:23.559049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 00:22:23.559066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-27 00:22:26.839677: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Epoch 1: Train SSE = 0.7631579041481018 and Test SSE = 0.7920871376991272\n",
            "Epoch 2: Train SSE = 0.6888033151626587 and Test SSE = 0.703596293926239\n",
            "Epoch 3: Train SSE = 0.6465884447097778 and Test SSE = 0.6648276448249817\n",
            "Epoch 4: Train SSE = 0.6219916343688965 and Test SSE = 0.6456611156463623\n",
            "Epoch 5: Train SSE = 0.6038587093353271 and Test SSE = 0.6329625248908997\n",
            "Epoch 6: Train SSE = 0.5891485214233398 and Test SSE = 0.6245360970497131\n",
            "Epoch 7: Train SSE = 0.5756386518478394 and Test SSE = 0.6176710724830627\n",
            "Epoch 8: Train SSE = 0.5625340938568115 and Test SSE = 0.6131972074508667\n",
            "Epoch 9: Train SSE = 0.5498656630516052 and Test SSE = 0.609352707862854\n",
            "Epoch 10: Train SSE = 0.5375365018844604 and Test SSE = 0.6068593263626099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "img = plt.imread('model_performance.png')\n",
        "plt.imshow(img)\n",
        "plt.axis(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "MhhHlSJr5unb",
        "outputId": "322601bc-e790-48e8-b271-691efd544ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommends Movies to the Specific User"
      ],
      "metadata": {
        "id": "8gjc2tqOYi69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile recommend.py\n",
        "\n",
        "import joblib\n",
        "from scipy.sparse import load_npz\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "# Create argument parser\n",
        "arg_parser = argparse.ArgumentParser()\n",
        "\n",
        "# Add arguments\n",
        "arg_parser.add_argument('--user_id', type = int, default = 1, help = 'Id of the user')\n",
        "arg_parser.add_argument('--top_n', type = int, default = 10, help = 'The number of movies to recommend' )\n",
        "\n",
        "# Parse argument\n",
        "args = arg_parser.parse_args()\n",
        "\n",
        "# Access the argument\n",
        "user_id = args.user_id\n",
        "n = args.top_n\n",
        "\n",
        "# Load model weights\n",
        "W, b, c = joblib.load('weights.pkl')\n",
        "K = 10\n",
        "ratings = load_npz('train_sparse.npz')\n",
        "N = ratings.shape[0]\n",
        "\n",
        "def dot1(V, W):\n",
        "    # V is N x D x K (batch of visible units)\n",
        "    # W is D x K x M (weights)\n",
        "    # returns N x M (hidden layer size)\n",
        "    return tf.tensordot(V, W, axes=[[1,2], [0,1]])\n",
        "\n",
        "def dot2(H, W):\n",
        "    # H is N x M (batch of hiddens)\n",
        "    # W is D x K x M (weights transposed)\n",
        "    # returns N x D x K (visible)\n",
        "    return tf.tensordot(H, W, axes=[[1], [2]])\n",
        "\n",
        "def forward_hidden(v):\n",
        "    return tf.nn.sigmoid(dot1(v, W) + c)\n",
        "\n",
        "def forward_logits(v):\n",
        "    Z = forward_hidden(v)\n",
        "    return dot2(Z, W) + b\n",
        "\n",
        "def forward_output(v):\n",
        "    return tf.nn.softmax(forward_logits(v))\n",
        "\n",
        "\n",
        "def generate_ranking(user_id):\n",
        "    # Get user ratings \n",
        "    user_rating = ratings[user_id,:].toarray()\n",
        "    user_input = tf.constant(user_rating*2-1, dtype = tf.int32) \n",
        "    # Convert to binary             \n",
        "    user_input_hot = tf.one_hot(user_input, K, dtype = tf.float32)   \n",
        "    # Predict user ratings for all movies                  \n",
        "    one_to_ten = tf.constant((np.arange(10) + 1).astype(np.float32) / 2)                \n",
        "    output_visible = forward_output(user_input_hot)\n",
        "    pred = tf.squeeze(tf.tensordot(output_visible, one_to_ten, axes=[[2], [0]]))\n",
        "    # Get the indices of movies already watched by the user\n",
        "    watched = tf.cast(tf.squeeze(user_rating)>0, dtype = tf.float32)\n",
        "\n",
        "    # Calculate MAE for the user rating\n",
        "    pred_watched = np.where(watched,pred,0)\n",
        "    diff = tf.abs(user_rating - pred_watched)\n",
        "    count = tf.reduce_sum(watched)\n",
        "    mae = tf.reduce_sum(diff).numpy()/count.numpy()\n",
        "    print(f'\\n\\nAverage prediction deviance for user {user_id} = {mae}')\n",
        "\n",
        "    # Rank the unwatched movies by rating in descending order\n",
        "    pred_not_watched = np.where(watched, 0, pred)\n",
        "    print( f'User has watched {int(tf.reduce_sum(watched).numpy())} movies in total')\n",
        "    movie_rating = {movie:rating for movie, rating in enumerate(pred_not_watched)}\n",
        "    top_movies = sorted(movie_rating, key = lambda x: movie_rating[x], reverse = True)\n",
        "    print(f'Top {n} new recommended movies (ids) for the user {user_id} is: {top_movies[:n]}')\n",
        "    return user_input, pred\n",
        "\n",
        "# Get top n recommendations\n",
        "rating_history, predicted_ratings = generate_ranking(user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBMYg0OJ808u",
        "outputId": "31a957fb-ac24-456b-ff34-1aebca6a3d47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing recommend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python recommend.py --user_id 5 --top_n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFasXfwZmybG",
        "outputId": "b5725f0d-d1f8-403f-a051-ae158a74783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-27 00:31:41.457239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-27 00:31:42.649834: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 00:31:42.649976: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-27 00:31:42.650001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-27 00:31:45.946674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\n",
            "\n",
            "Average prediction deviance for user 5 = 0.4997832969104837\n",
            "User has watched 381 movies in total\n",
            "Top 20 new recommended movies (ids) for the user 5 is: [468, 356, 409, 620, 500, 594, 901, 586, 493, 604, 168, 484, 598, 883, 924, 612, 638, 310, 763, 509]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cCwYecBYm7yh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}